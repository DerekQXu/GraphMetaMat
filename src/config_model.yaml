forward_model:
  dim: 128 # 64
  encoder_cfg:
    n_layers: 4 #4
    conv_type: MPNN #EGNN #MPNN #EGNN #MPNN #IGNN #MPNN #crystal #MPNN #GCN #GAT
    norm_type: GraphNorm # GraphNorm BatchNorm
    act: SiLU # ELU
    skip: true
    dropout: 0.0
  pooler_cfg:
    merge_name: CAT
    pool_name: ATT #SUM
  decoder_shape_nm: CNNLSTMDecoderOriginal #DeepONet #CNNLSTMDecoderOriginal
  decoder_shape_cfg:
    n_layers_cnn: 2 #6 [1,8]
    use_nonlinearity: True # [F,T]
    n_start_tokens: 4 # [1,256]
    kernel_size: 8 # [1,8]
    n_layers_rnn: 1 # [1,8]
    module_rnn: GRU #LSTM
    stride: 8
  decoder_magnitude_nm: MLPDecoder
  decoder_magitude_cfg:
    n_layers: 5 #5
    dropout: 0.0 #0.28
    use_skip: true
    use_norm: true
    act: SiLU
  loss_name: mse
  loss_coeff:
    main_coeff: 0.0 #10.0
    magnitude_coeff: 0.6 #0.0 # 0.6
    shape_coeff: 0.4 #1.0 # 0.4
    smoothness_coeff: 0.0 # 0.0002 # 0.001 # 0.001

policy_network:
  dim: 256
  encoder_graph_cfg:
    n_layers: 3 # asdf 3
    conv_type: GAT #EGNN #IGNN #MPNN #crystal #MPNN #GCN #GAT
    norm_type: GraphNorm # GraphNorm BatchNorm
    act: SiLU # ELU
    skip: true
    dropout: 0.0
  encoder_curve_cfg:
    mlp_magnitude_cfg:
      n_layers: 7
      dropout: 0.0 #0.35
      use_skip: true
      use_norm: true #true
      act: SiLU
    cnn_shape_cfg:
      use_norm: true #true
    mlp_shape_magnitude_cfg:
      ins: both
#      ins: shape
      n_layers: 1
      dropout: 0.0 #0.35
      use_skip: true
      use_norm: true #true
      act: SiLU
#  pooler_cfg:
#    merge_inputs:
#      nodes: ATT
#      edges: null
#      rho: null
#    merge_method: null
  mlp_state_action_cfg:
    n_layers: 1
    dropout: 0.0
    use_skip: true
    use_norm: false
    act: SiLU
  readout_start_cfg:
    n_layers: 2
    dropout: 0.0
    use_skip: true
    use_norm: false
    act: SiLU
  readout_stop_cfg:
    n_layers: 2
    dropout: 0.0
    use_skip: true
    use_norm: false
    act: SiLU
  readout_end_cfg:
    n_layers: 2
    dropout: 0.0
    use_skip: true
    use_norm: false
    act: SiLU
  readout_rho_cfg:
    n_layers: 5
    dropout: 0.0
    use_skip: true
    use_norm: false
    act: SiLU